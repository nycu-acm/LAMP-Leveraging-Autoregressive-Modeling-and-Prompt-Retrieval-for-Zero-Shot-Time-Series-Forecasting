{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b901654",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Đặt OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def extract_features_with_llm_from_file(input_txt_path: str, output_json_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Đọc mô tả từ file text, trích xuất metadata bằng LLM và lưu kết quả ra file JSON.\n",
    "    Nếu file JSON đã tồn tại, không gọi lại API.\n",
    "    \"\"\"\n",
    "    # Nếu đã tồn tại file JSON, đọc lại\n",
    "    if os.path.exists(output_json_path):\n",
    "        print(f\"[INFO] File kết quả đã tồn tại: '{output_json_path}', không gọi API.\")\n",
    "        with open(output_json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Đọc mô tả từ file text\n",
    "    if not os.path.exists(input_txt_path):\n",
    "        print(f\"[ERROR] Không tìm thấy file mô tả: {input_txt_path}\")\n",
    "        return {}\n",
    "\n",
    "    with open(input_txt_path, 'r', encoding='utf-8') as f:\n",
    "        description = f.read().strip()\n",
    "\n",
    "    # Tạo prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant that extracts structured metadata from a time series dataset description.\n",
    "\n",
    "Here is the input description:\n",
    "\\\"\\\"\\\"{description}\\\"\\\"\\\"\n",
    "\n",
    "Extract the following fields:\n",
    "- Sampling rate (e.g., 1 minute, hourly, daily)\n",
    "- Duration (e.g., 4 years, 24 months)\n",
    "- Seasonality type (daily / weekly / yearly / unknown)\n",
    "Return the result in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "    # Gọi API\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        metadata = json.loads(content)\n",
    "\n",
    "        # Ghi ra file kết quả\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"[INFO] Kết quả đã được lưu tại: '{output_json_path}'\")\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"[ERROR] JSON không hợp lệ từ phản hồi mô hình:\")\n",
    "        print(content)\n",
    "        return {}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Gặp lỗi khi gọi API hoặc xử lý: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a1897",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metadata = extract_features_with_llm_from_file(\"text.txt\", \"metadata_output.json\")\n",
    "print(json.dumps(metadata, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24e5f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Auto STL-Prompt Generator Pipeline\n",
    "----------------------------------\n",
    "This script reads a dataset description from a .txt file, uses OpenAI GPT-4 to extract\n",
    "metadata, and then generates STL-based prefix prompts for trend/seasonal/residual forecasting.\n",
    "\n",
    "Usage:\n",
    "    1. Set your OpenAI API key in the OPENAI_API_KEY environment variable or directly below.\n",
    "    2. Prepare .txt files with your dataset descriptions (e.g., ETT.txt, Weather.txt, etc.).\n",
    "    3. Provide recent STL values (trend, seasonal, residual) for each dataset.\n",
    "    4. Run this script: python auto_stl_prompt.py\n",
    "\n",
    "Requirements:\n",
    "    - Python 3.7+\n",
    "    - openai (pip install openai)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from typing import Dict, List\n",
    "\n",
    "# ----------------------------- #\n",
    "#  Configuration / Parameters  #\n",
    "# ----------------------------- #\n",
    "\n",
    "# You can set your API key as an environment variable or directly assign here:\n",
    "# export OPENAI_API_KEY=\"your-api-key\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# List of dataset description files (adjust paths as needed)\n",
    "DATASET_FILES = [\n",
    "    \"ETTh.txt\",\n",
    "    \"ETTm.txt\",\n",
    "    \"Weather.txt\",\n",
    "    \"ECL.txt\",\n",
    "    \"ILI.txt\",\n",
    "    \"m4.txt\",\n",
    "    \"Traffic.txt\"\n",
    "]\n",
    "\n",
    "# Example recent STL values for demonstration (replace with actual values per dataset)\n",
    "EXAMPLE_RECENT_VALUES = {\n",
    "    \"trend\": [1.0, 1.1, 1.2, 1.15, 1.18],\n",
    "    \"seasonal\": [0.3, 0.35, 0.32, 0.33, 0.34],\n",
    "    \"residual\": [-0.05, 0.0, 0.02, -0.01, 0.01]\n",
    "}\n",
    "\n",
    "# Output folder for saving prompts (will be created if missing)\n",
    "OUTPUT_FOLDER = \"prompt_bank\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "#       Helper Functions        #\n",
    "# ----------------------------- #\n",
    "\n",
    "def read_description_from_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read the dataset description from a .txt file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "def extract_metadata_from_description(description: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Use GPT-4 to extract structured metadata from a dataset description.\n",
    "    Returns a dictionary with fields:\n",
    "      - dataset_name\n",
    "      - sampling_rate\n",
    "      - duration\n",
    "      - seasonality_type\n",
    "      - number_of_features\n",
    "      - target_variable\n",
    "      - domain\n",
    "      - train_val_test_split\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Extract structured metadata from the following time series dataset description. Return only a JSON object with the following fields:\n",
    "- dataset_name\n",
    "- sampling_rate\n",
    "- duration\n",
    "- seasonality_type (daily / weekly / yearly / unknown)\n",
    "- number_of_features\n",
    "- target_variable\n",
    "- domain\n",
    "- train_val_test_split\n",
    "\n",
    "Description:\n",
    "\\\"\\\"\\\"{description}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        metadata = json.loads(content)\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract metadata: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def generate_prefix_prompts(metadata: Dict) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate STL-based prefix prompts (trend, seasonal, residual) from metadata.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"trend\": (\n",
    "            f\"You are forecasting the long-term trend of {metadata['target_variable']} in a {metadata['domain']} dataset \"\n",
    "            f\"called {metadata['dataset_name']}, sampled every {metadata['sampling_rate']} over {metadata['duration']}. \"\n",
    "            f\"Focus on modeling smooth and gradual structural changes over time.\"\n",
    "        ),\n",
    "        \"seasonal\": (\n",
    "            f\"You are forecasting the {metadata['seasonality_type']} seasonal variation of {metadata['target_variable']} \"\n",
    "            f\"in the {metadata['domain']} dataset {metadata['dataset_name']}, sampled every {metadata['sampling_rate']}. \"\n",
    "            f\"Focus on modeling repeating cycles or periodic patterns in the data.\"\n",
    "        ),\n",
    "        \"residual\": (\n",
    "            f\"You are estimating short-term irregular fluctuations in {metadata['target_variable']} in the dataset \"\n",
    "            f\"{metadata['dataset_name']} after removing trend and {metadata['seasonality_type']} seasonality. \"\n",
    "            f\"The data is sampled at {metadata['sampling_rate']} and spans {metadata['duration']}. Focus on modeling noise and unexpected spikes.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_full_prompts(metadata: Dict, values: Dict[str, List[float]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Combine prefix prompts with recent STL values to form the full prompt.\n",
    "    \"\"\"\n",
    "    prefixes = generate_prefix_prompts(metadata)\n",
    "    return {\n",
    "        component: f\"{prefixes[component]}\"\n",
    "        for component in [\"trend\", \"seasonal\", \"residual\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "#           Main Logic          #\n",
    "# ----------------------------- #\n",
    "\n",
    "def run_pipeline(txt_file_path: str, recent_values: Dict[str, List[float]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Execute the full pipeline:\n",
    "      1. Read description from .txt\n",
    "      2. Extract metadata using GPT-4\n",
    "      3. Generate STL-based prompts\n",
    "    Returns a dict of prompts for 'trend', 'seasonal', 'residual'.\n",
    "    \"\"\"\n",
    "    description = read_description_from_file(txt_file_path)\n",
    "    metadata = extract_metadata_from_description(description)\n",
    "\n",
    "    if not metadata:\n",
    "        return {}\n",
    "\n",
    "    prompts = generate_full_prompts(metadata, recent_values)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Iterate over the dataset files, run pipeline, and save prompts to disk.\n",
    "    \"\"\"\n",
    "    for file_name in DATASET_FILES:\n",
    "        print(f\"[INFO] Processing file: {file_name}\")\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"[WARN] File not found: {file_name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing: {file_name}\")\n",
    "        prompts = run_pipeline(file_name, EXAMPLE_RECENT_VALUES)\n",
    "\n",
    "        if not prompts:\n",
    "            print(f\"[ERROR] Pipeline failed for {file_name}.\")\n",
    "            continue\n",
    "\n",
    "        # Save prompts as JSON\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, f\"{base_name}_prompts.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(prompts, f, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Saved prompts to {output_path}\")\n",
    "\n",
    "        # Print prompts to console\n",
    "        for component, prompt in prompts.items():\n",
    "            print(f\"\\n--- {base_name.upper()} {component.upper()} PROMPT ---\")\n",
    "            print(prompt)\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
